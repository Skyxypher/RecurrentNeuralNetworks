{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPZQIp8CBCI9WnRt3M4H3KB",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Skyxypher/RecurrentNeuralNetworks/blob/main/RecurrentNeuralNetwork.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Group Members\n",
        "1. Michael Wanjala   IN13/00025/21\n",
        "2. Finney Abuko      IN13/00027/21\n",
        "3. Brian Ayoma       IN13/00076/21"
      ],
      "metadata": {
        "id": "QiM_tNC4nomb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('gutenberg')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7xD0HUCv69p5",
        "outputId": "29c30eb7-8970-4d8b-9e95-ff272247d0b1"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package gutenberg to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/gutenberg.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from __future__ import print_function\n",
        "import sys\n",
        "import numpy as np\n",
        "import re\n",
        "import random\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, SimpleRNN, GRU, Embedding, Dropout\n",
        "from tensorflow.keras.optimizers import RMSprop\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from nltk.corpus import gutenberg\n",
        "\n",
        "# Load and preprocess Shakespeare's Hamlet\n",
        "gutenberg.ensure_loaded()\n",
        "text = ''.join(gutenberg.words('shakespeare-hamlet.txt')).lower()\n",
        "\n",
        "# Create character dictionary\n",
        "chars = sorted(list(set(text)))\n",
        "char_indices = {c: i for i, c in enumerate(chars)}\n",
        "indices_char = {i: c for i, c in enumerate(chars)}\n",
        "\n",
        "# Create sequences for training\n",
        "seq_length = 40\n",
        "step = 3\n",
        "dataX, dataY = [], []\n",
        "for i in range(0, len(text) - seq_length, step):\n",
        "    dataX.append([char_indices[c] for c in text[i:i + seq_length]])\n",
        "    dataY.append(char_indices[text[i + seq_length]])\n",
        "\n",
        "# Convert to numpy arrays\n",
        "X = np.array(dataX)\n",
        "y = to_categorical(dataY, num_classes=len(chars))\n",
        "\n",
        "# Define the RNN model\n",
        "def build_rnn_model():\n",
        "    model = Sequential([\n",
        "        Embedding(len(chars), 50),\n",
        "        SimpleRNN(256, activation='tanh', return_sequences=False),\n",
        "        Dropout(0.3),\n",
        "        Dense(len(chars), activation='softmax')\n",
        "    ])\n",
        "    model.compile(loss='categorical_crossentropy', optimizer=RMSprop(learning_rate=0.005))\n",
        "    return model\n",
        "\n",
        "# Define the GRU model\n",
        "def build_gru_model():\n",
        "    model = Sequential([\n",
        "        Embedding(len(chars), 50),\n",
        "        GRU(256, activation='tanh', return_sequences=False),\n",
        "        Dropout(0.3),\n",
        "        Dense(len(chars), activation='softmax')\n",
        "    ])\n",
        "    model.compile(loss='categorical_crossentropy', optimizer=RMSprop(learning_rate=0.005))\n",
        "    return model\n",
        "\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "\n",
        "# Define Early Stopping to prevent overfitting\n",
        "early_stop = EarlyStopping(monitor='loss', patience=5, restore_best_weights=True)\n",
        "\n",
        "# Train both models\n",
        "rnn_model = build_rnn_model()\n",
        "gru_model = build_gru_model()\n",
        "rnn_model.fit(X, y, batch_size=128, epochs=30, callbacks=[early_stop])\n",
        "gru_model.fit(X, y, batch_size=128, epochs=30, callbacks=[early_stop])\n",
        "\n",
        "# Function to generate text from trained model\n",
        "import numpy as np\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "def generate_text(model, seed_text, length=200, temperature=0.8):\n",
        "    result = seed_text\n",
        "    for _ in range(length):\n",
        "        input_seq = np.array([[char_indices[c] for c in result[-seq_length:]]])\n",
        "        preds = model.predict(input_seq, verbose=0)[0]\n",
        "        next_char = indices_char[np.argmax(preds)]\n",
        "        result += next_char\n",
        "    return result\n",
        "\n",
        "# Example text generation\n",
        "seed = text[:40]\n",
        "print(\"Generated by RNN:\\n\", generate_text(rnn_model, seed))\n",
        "print(\"Generated by GRU:\\n\", generate_text(gru_model, seed))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ghxZK4ItRhbA",
        "outputId": "7fa36074-383e-4aaa-baf8-bb5850635474"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/30\n",
            "\u001b[1m337/337\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 88ms/step - loss: 3.5984\n",
            "Epoch 2/30\n",
            "\u001b[1m337/337\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 91ms/step - loss: 2.9335\n",
            "Epoch 3/30\n",
            "\u001b[1m337/337\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 88ms/step - loss: 2.8484\n",
            "Epoch 4/30\n",
            "\u001b[1m337/337\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 88ms/step - loss: 2.8893\n",
            "Epoch 5/30\n",
            "\u001b[1m337/337\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 91ms/step - loss: 2.7807\n",
            "Epoch 6/30\n",
            "\u001b[1m337/337\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 91ms/step - loss: 2.7929\n",
            "Epoch 7/30\n",
            "\u001b[1m337/337\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 88ms/step - loss: 2.7463\n",
            "Epoch 8/30\n",
            "\u001b[1m337/337\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 88ms/step - loss: 2.7781\n",
            "Epoch 9/30\n",
            "\u001b[1m337/337\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 92ms/step - loss: 2.7768\n",
            "Epoch 10/30\n",
            "\u001b[1m337/337\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 91ms/step - loss: 2.7800\n",
            "Epoch 11/30\n",
            "\u001b[1m337/337\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 88ms/step - loss: 2.7937\n",
            "Epoch 12/30\n",
            "\u001b[1m337/337\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 88ms/step - loss: 2.7810\n",
            "Epoch 1/30\n",
            "\u001b[1m337/337\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m96s\u001b[0m 274ms/step - loss: 2.8637\n",
            "Epoch 2/30\n",
            "\u001b[1m337/337\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m141s\u001b[0m 273ms/step - loss: 2.4828\n",
            "Epoch 3/30\n",
            "\u001b[1m337/337\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m141s\u001b[0m 271ms/step - loss: 2.3160\n",
            "Epoch 4/30\n",
            "\u001b[1m337/337\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m142s\u001b[0m 271ms/step - loss: 2.2050\n",
            "Epoch 5/30\n",
            "\u001b[1m337/337\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m142s\u001b[0m 271ms/step - loss: 2.0931\n",
            "Epoch 6/30\n",
            "\u001b[1m337/337\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m141s\u001b[0m 267ms/step - loss: 2.0012\n",
            "Epoch 7/30\n",
            "\u001b[1m337/337\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m91s\u001b[0m 269ms/step - loss: 1.9250\n",
            "Epoch 8/30\n",
            "\u001b[1m337/337\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m142s\u001b[0m 269ms/step - loss: 1.8576\n",
            "Epoch 9/30\n",
            "\u001b[1m337/337\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m141s\u001b[0m 267ms/step - loss: 1.8190\n",
            "Epoch 10/30\n",
            "\u001b[1m337/337\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m90s\u001b[0m 267ms/step - loss: 1.8019\n",
            "Epoch 11/30\n",
            "\u001b[1m337/337\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m145s\u001b[0m 276ms/step - loss: 1.7737\n",
            "Epoch 12/30\n",
            "\u001b[1m337/337\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m140s\u001b[0m 271ms/step - loss: 1.7823\n",
            "Epoch 13/30\n",
            "\u001b[1m337/337\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m91s\u001b[0m 271ms/step - loss: 1.7963\n",
            "Epoch 14/30\n",
            "\u001b[1m337/337\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m142s\u001b[0m 270ms/step - loss: 1.7971\n",
            "Epoch 15/30\n",
            "\u001b[1m337/337\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m142s\u001b[0m 270ms/step - loss: 1.8200\n",
            "Epoch 16/30\n",
            "\u001b[1m337/337\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m91s\u001b[0m 271ms/step - loss: 1.8244\n",
            "Epoch 17/30\n",
            "\u001b[1m337/337\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m142s\u001b[0m 271ms/step - loss: 1.8485\n",
            "Generated by RNN:\n",
            " [thetragedieofhamletbywilliamshakespearethesinthatithesinthatithesithesinthatithesinthatithesinthatithesinthatheretherethesintherethesinthatithesinthatherethesinthatithesinthatitherethesintherethesinthatithesinthatithesithesinthatithesintha\n",
            "Generated by GRU:\n",
            " [thetragedieofhamletbywilliamshakespeareistotheconsandfalleflonthesersenther.whatisthemosttheneassestoftheseefeinthemosttheseand,andtheseluesttheseath,andthesearesofthediddowne,thesearesthemadnessesthathersenther.mylordham.thereadestheseand\n"
          ]
        }
      ]
    }
  ]
}